#!/usr/bin/env python3
"""
æ•°æ®è½¬æ¢å·¥å…·
æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ä¹‹é—´çš„è½¬æ¢ï¼šJSON, CSV, XML, YAML, Excelç­‰
"""

import os
import json
import csv
import xml.etree.ElementTree as ET
import argparse
from pathlib import Path
from typing import Dict, List, Any, Union
import pandas as pd
import yaml
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel

console = Console()


class DataConverter:
    """æ•°æ®è½¬æ¢å·¥å…·"""
    
    SUPPORTED_FORMATS = {
        'json': ['.json'],
        'csv': ['.csv'],
        'xml': ['.xml'],
        'yaml': ['.yaml', '.yml'],
        'excel': ['.xlsx', '.xls'],
        'tsv': ['.tsv'],
        'parquet': ['.parquet'],
        'html': ['.html', '.htm']
    }
    
    def __init__(self):
        self.converted_files = []
        self.errors = []
        self.operations_log = []
    
    def detect_format(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        extension = Path(file_path).suffix.lower()
        
        for format_name, extensions in self.SUPPORTED_FORMATS.items():
            if extension in extensions:
                return format_name
        
        return 'unknown'
    
    def read_json(self, file_path: str) -> Union[Dict, List]:
        """è¯»å–JSONæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def read_csv(self, file_path: str, **kwargs) -> pd.DataFrame:
        """è¯»å–CSVæ–‡ä»¶"""
        return pd.read_csv(file_path, **kwargs)
    
    def read_xml(self, file_path: str) -> Dict:
        """è¯»å–XMLæ–‡ä»¶"""
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        def xml_to_dict(element):
            result = {}
            
            # æ·»åŠ å±æ€§
            if element.attrib:
                result['@attributes'] = element.attrib
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if element.text and element.text.strip():
                if len(element) == 0:
                    return element.text.strip()
                result['#text'] = element.text.strip()
            
            # æ·»åŠ å­å…ƒç´ 
            for child in element:
                child_data = xml_to_dict(child)
                if child.tag in result:
                    if not isinstance(result[child.tag], list):
                        result[child.tag] = [result[child.tag]]
                    result[child.tag].append(child_data)
                else:
                    result[child.tag] = child_data
            
            return result
        
        return {root.tag: xml_to_dict(root)}
    
    def read_yaml(self, file_path: str) -> Union[Dict, List]:
        """è¯»å–YAMLæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def read_excel(self, file_path: str, **kwargs) -> pd.DataFrame:
        """è¯»å–Excelæ–‡ä»¶"""
        return pd.read_excel(file_path, **kwargs)
    
    def read_tsv(self, file_path: str, **kwargs) -> pd.DataFrame:
        """è¯»å–TSVæ–‡ä»¶"""
        return pd.read_csv(file_path, sep='\t', **kwargs)
    
    def read_parquet(self, file_path: str) -> pd.DataFrame:
        """è¯»å–Parquetæ–‡ä»¶"""
        return pd.read_parquet(file_path)
    
    def write_json(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥JSONæ–‡ä»¶"""
        # å¦‚æœæ•°æ®æ˜¯DataFrameï¼Œè½¬æ¢ä¸ºå­—å…¸
        if isinstance(data, pd.DataFrame):
            data = data.to_dict('records')
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, **kwargs)
    
    def write_csv(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥CSVæ–‡ä»¶"""
        if isinstance(data, pd.DataFrame):
            data.to_csv(file_path, index=False, **kwargs)
        elif isinstance(data, list) and data and isinstance(data[0], dict):
            df = pd.DataFrame(data)
            df.to_csv(file_path, index=False, **kwargs)
        else:
            raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒè½¬æ¢ä¸ºCSV")
    
    def write_xml(self, data: Any, file_path: str, root_name: str = 'root'):
        """å†™å…¥XMLæ–‡ä»¶"""
        def dict_to_xml(parent, data, item_name='item'):
            if isinstance(data, dict):
                for key, value in data.items():
                    if key.startswith('@'):
                        continue
                    elif key == '#text':
                        parent.text = str(value)
                    else:
                        child = ET.SubElement(parent, key)
                        dict_to_xml(child, value, key)
            elif isinstance(data, list):
                for item in data:
                    child = ET.SubElement(parent, item_name)
                    dict_to_xml(child, item, item_name)
            else:
                parent.text = str(data)
        
        # å¦‚æœæ•°æ®æ˜¯DataFrameï¼Œè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        if isinstance(data, pd.DataFrame):
            data = data.to_dict('records')
        
        root = ET.Element(root_name)
        dict_to_xml(root, data)
        
        tree = ET.ElementTree(root)
        tree.write(file_path, encoding='utf-8', xml_declaration=True)
    
    def write_yaml(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥YAMLæ–‡ä»¶"""
        # å¦‚æœæ•°æ®æ˜¯DataFrameï¼Œè½¬æ¢ä¸ºå­—å…¸
        if isinstance(data, pd.DataFrame):
            data = data.to_dict('records')
        
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True, **kwargs)
    
    def write_excel(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥Excelæ–‡ä»¶"""
        if isinstance(data, pd.DataFrame):
            data.to_excel(file_path, index=False, **kwargs)
        elif isinstance(data, list) and data and isinstance(data[0], dict):
            df = pd.DataFrame(data)
            df.to_excel(file_path, index=False, **kwargs)
        else:
            raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒè½¬æ¢ä¸ºExcel")
    
    def write_tsv(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥TSVæ–‡ä»¶"""
        if isinstance(data, pd.DataFrame):
            data.to_csv(file_path, sep='\t', index=False, **kwargs)
        elif isinstance(data, list) and data and isinstance(data[0], dict):
            df = pd.DataFrame(data)
            df.to_csv(file_path, sep='\t', index=False, **kwargs)
        else:
            raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒè½¬æ¢ä¸ºTSV")
    
    def write_parquet(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥Parquetæ–‡ä»¶"""
        if isinstance(data, pd.DataFrame):
            data.to_parquet(file_path, **kwargs)
        elif isinstance(data, list) and data and isinstance(data[0], dict):
            df = pd.DataFrame(data)
            df.to_parquet(file_path, **kwargs)
        else:
            raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒè½¬æ¢ä¸ºParquet")
    
    def write_html(self, data: Any, file_path: str, **kwargs):
        """å†™å…¥HTMLæ–‡ä»¶"""
        if isinstance(data, pd.DataFrame):
            data.to_html(file_path, index=False, **kwargs)
        elif isinstance(data, list) and data and isinstance(data[0], dict):
            df = pd.DataFrame(data)
            df.to_html(file_path, index=False, **kwargs)
        else:
            raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒè½¬æ¢ä¸ºHTML")
    
    def convert_file(self, input_file: str, output_file: str, **kwargs) -> bool:
        """è½¬æ¢å•ä¸ªæ–‡ä»¶"""
        try:
            input_format = self.detect_format(input_file)
            output_format = self.detect_format(output_file)
            
            if input_format == 'unknown':
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ–‡ä»¶æ ¼å¼: {input_file}")
            
            if output_format == 'unknown':
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ–‡ä»¶æ ¼å¼: {output_file}")
            
            # è¯»å–æ•°æ®
            read_method = getattr(self, f'read_{input_format}')
            data = read_method(input_file, **kwargs.get('read_options', {}))
            
            # å†™å…¥æ•°æ®
            write_method = getattr(self, f'write_{output_format}')
            write_method(data, output_file, **kwargs.get('write_options', {}))
            
            self.converted_files.append({
                'input': input_file,
                'output': output_file,
                'input_format': input_format,
                'output_format': output_format
            })
            
            self.operations_log.append(f"è½¬æ¢: {input_file} -> {output_file}")
            return True
            
        except Exception as e:
            error_msg = f"è½¬æ¢æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}"
            self.errors.append(error_msg)
            return False
    
    def batch_convert(self, input_dir: str, output_dir: str, 
                     input_format: str, output_format: str, **kwargs) -> Dict:
        """æ‰¹é‡è½¬æ¢æ–‡ä»¶"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        if not input_path.exists():
            error_msg = f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            self.errors.append(error_msg)
            return {'converted': 0, 'errors': [error_msg]}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path.mkdir(parents=True, exist_ok=True)
        
        # è·å–è¾“å…¥æ–‡ä»¶
        input_extensions = self.SUPPORTED_FORMATS.get(input_format, [])
        input_files = []
        
        for ext in input_extensions:
            input_files.extend(input_path.glob(f'*{ext}'))
        
        if not input_files:
            error_msg = f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ° {input_format} æ ¼å¼çš„æ–‡ä»¶"
            self.errors.append(error_msg)
            return {'converted': 0, 'errors': [error_msg]}
        
        converted_count = 0
        errors = []
        
        # è·å–è¾“å‡ºæ‰©å±•å
        output_ext = self.SUPPORTED_FORMATS[output_format][0]
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("ğŸ”„ æ‰¹é‡è½¬æ¢", total=len(input_files))
            
            for input_file in input_files:
                try:
                    progress.update(task, description=f"è½¬æ¢: {input_file.name}")
                    
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                    output_file = output_path / f"{input_file.stem}{output_ext}"
                    
                    if self.convert_file(str(input_file), str(output_file), **kwargs):
                        converted_count += 1
                    else:
                        errors.extend(self.errors[-1:])  # æ·»åŠ æœ€æ–°çš„é”™è¯¯
                    
                except Exception as e:
                    error_msg = f"å¤„ç†æ–‡ä»¶ {input_file.name} æ—¶å‡ºé”™: {str(e)}"
                    errors.append(error_msg)
                    self.errors.append(error_msg)
                
                progress.advance(task)
        
        return {
            'converted': converted_count,
            'total_files': len(input_files),
            'errors': errors,
            'success_rate': f"{(converted_count / len(input_files) * 100):.1f}%"
        }
    
    def merge_files(self, input_files: List[str], output_file: str, **kwargs) -> bool:
        """åˆå¹¶å¤šä¸ªæ–‡ä»¶"""
        try:
            all_data = []
            
            for input_file in input_files:
                input_format = self.detect_format(input_file)
                if input_format == 'unknown':
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")
                
                read_method = getattr(self, f'read_{input_format}')
                data = read_method(input_file)
                
                # ç»Ÿä¸€æ•°æ®æ ¼å¼
                if isinstance(data, pd.DataFrame):
                    all_data.append(data)
                elif isinstance(data, list):
                    all_data.extend(data)
                elif isinstance(data, dict):
                    all_data.append(data)
            
            # åˆå¹¶æ•°æ®
            if all(isinstance(d, pd.DataFrame) for d in all_data):
                merged_data = pd.concat(all_data, ignore_index=True)
            else:
                merged_data = all_data
            
            # å†™å…¥åˆå¹¶åçš„æ–‡ä»¶
            output_format = self.detect_format(output_file)
            write_method = getattr(self, f'write_{output_format}')
            write_method(merged_data, output_file, **kwargs)
            
            self.operations_log.append(f"åˆå¹¶: {len(input_files)} ä¸ªæ–‡ä»¶ -> {output_file}")
            return True
            
        except Exception as e:
            error_msg = f"åˆå¹¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
            self.errors.append(error_msg)
            return False
    
    def split_file(self, input_file: str, output_dir: str, 
                   split_by: str = 'rows', chunk_size: int = 1000, **kwargs) -> Dict:
        """æ‹†åˆ†æ–‡ä»¶"""
        try:
            input_format = self.detect_format(input_file)
            if input_format == 'unknown':
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")
            
            # è¯»å–æ•°æ®
            read_method = getattr(self, f'read_{input_format}')
            data = read_method(input_file)
            
            if not isinstance(data, pd.DataFrame):
                if isinstance(data, list):
                    data = pd.DataFrame(data)
                else:
                    raise ValueError("æ•°æ®æ ¼å¼ä¸æ”¯æŒæ‹†åˆ†")
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            input_path = Path(input_file)
            base_name = input_path.stem
            extension = input_path.suffix
            
            split_files = []
            
            if split_by == 'rows':
                # æŒ‰è¡Œæ•°æ‹†åˆ†
                total_chunks = (len(data) + chunk_size - 1) // chunk_size
                
                for i in range(total_chunks):
                    start_idx = i * chunk_size
                    end_idx = min((i + 1) * chunk_size, len(data))
                    chunk_data = data.iloc[start_idx:end_idx]
                    
                    output_file = output_path / f"{base_name}_part_{i+1:03d}{extension}"
                    
                    # å†™å…¥æ•°æ®å—
                    write_method = getattr(self, f'write_{input_format}')
                    write_method(chunk_data, str(output_file))
                    
                    split_files.append(str(output_file))
            
            elif split_by == 'column':
                # æŒ‰åˆ—æ‹†åˆ†
                column_name = kwargs.get('column_name')
                if not column_name or column_name not in data.columns:
                    raise ValueError(f"åˆ— '{column_name}' ä¸å­˜åœ¨")
                
                unique_values = data[column_name].unique()
                
                for value in unique_values:
                    chunk_data = data[data[column_name] == value]
                    safe_value = str(value).replace('/', '_').replace('\\', '_')
                    output_file = output_path / f"{base_name}_{safe_value}{extension}"
                    
                    # å†™å…¥æ•°æ®å—
                    write_method = getattr(self, f'write_{input_format}')
                    write_method(chunk_data, str(output_file))
                    
                    split_files.append(str(output_file))
            
            self.operations_log.append(f"æ‹†åˆ†: {input_file} -> {len(split_files)} ä¸ªæ–‡ä»¶")
            
            return {
                'split_files': split_files,
                'total_files': len(split_files),
                'success': True
            }
            
        except Exception as e:
            error_msg = f"æ‹†åˆ†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}"
            self.errors.append(error_msg)
            return {'success': False, 'error': error_msg}
    
    def get_file_info(self, file_path: str) -> Dict:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            file_format = self.detect_format(file_path)
            if file_format == 'unknown':
                return {'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}
            
            read_method = getattr(self, f'read_{file_format}')
            data = read_method(file_path)
            
            info = {
                'file_path': file_path,
                'format': file_format,
                'size': os.path.getsize(file_path)
            }
            
            if isinstance(data, pd.DataFrame):
                info.update({
                    'rows': len(data),
                    'columns': len(data.columns),
                    'column_names': list(data.columns),
                    'data_types': data.dtypes.to_dict()
                })
            elif isinstance(data, list):
                info.update({
                    'items': len(data),
                    'type': 'list'
                })
            elif isinstance(data, dict):
                info.update({
                    'keys': list(data.keys()),
                    'type': 'dict'
                })
            
            return info
            
        except Exception as e:
            return {'error': f"è¯»å–æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"}
    
    def get_report(self) -> Dict:
        """è·å–è½¬æ¢æŠ¥å‘Š"""
        return {
            'converted_files': self.converted_files,
            'errors': self.errors,
            'operations_log': self.operations_log,
            'total_converted': len(self.converted_files),
            'total_errors': len(self.errors)
        }
    
    def display_report(self):
        """æ˜¾ç¤ºè½¬æ¢æŠ¥å‘Š"""
        report = self.get_report()
        
        # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
        table = Table(title="ğŸ“Š æ•°æ®è½¬æ¢æŠ¥å‘Š")
        table.add_column("é¡¹ç›®", style="cyan")
        table.add_column("æ•°é‡", style="magenta")
        
        table.add_row("è½¬æ¢æˆåŠŸ", str(report['total_converted']))
        table.add_row("è½¬æ¢å¤±è´¥", str(report['total_errors']))
        table.add_row("æ€»æ“ä½œæ•°", str(len(report['operations_log'])))
        
        console.print(table)
        
        # æ˜¾ç¤ºæ“ä½œæ—¥å¿—
        if report['operations_log']:
            console.print("\nğŸ“ æ“ä½œæ—¥å¿—:")
            for log in report['operations_log'][-10:]:  # æ˜¾ç¤ºæœ€å10æ¡
                console.print(f"  â€¢ {log}")
            
            if len(report['operations_log']) > 10:
                console.print(f"  ... è¿˜æœ‰ {len(report['operations_log']) - 10} æ¡è®°å½•")
        
        # æ˜¾ç¤ºé”™è¯¯
        if report['errors']:
            console.print("\nâŒ é”™è¯¯ä¿¡æ¯:")
            for error in report['errors']:
                console.print(f"  â€¢ {error}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®è½¬æ¢å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è½¬æ¢å‘½ä»¤
    convert_parser = subparsers.add_parser('convert', help='è½¬æ¢å•ä¸ªæ–‡ä»¶')
    convert_parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶')
    convert_parser.add_argument('output_file', help='è¾“å‡ºæ–‡ä»¶')
    
    # æ‰¹é‡è½¬æ¢å‘½ä»¤
    batch_parser = subparsers.add_parser('batch', help='æ‰¹é‡è½¬æ¢æ–‡ä»¶')
    batch_parser.add_argument('input_dir', help='è¾“å…¥ç›®å½•')
    batch_parser.add_argument('output_dir', help='è¾“å‡ºç›®å½•')
    batch_parser.add_argument('input_format', help='è¾“å…¥æ ¼å¼')
    batch_parser.add_argument('output_format', help='è¾“å‡ºæ ¼å¼')
    
    # åˆå¹¶å‘½ä»¤
    merge_parser = subparsers.add_parser('merge', help='åˆå¹¶æ–‡ä»¶')
    merge_parser.add_argument('output_file', help='è¾“å‡ºæ–‡ä»¶')
    merge_parser.add_argument('input_files', nargs='+', help='è¾“å…¥æ–‡ä»¶åˆ—è¡¨')
    
    # æ‹†åˆ†å‘½ä»¤
    split_parser = subparsers.add_parser('split', help='æ‹†åˆ†æ–‡ä»¶')
    split_parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶')
    split_parser.add_argument('output_dir', help='è¾“å‡ºç›®å½•')
    split_parser.add_argument('--by', choices=['rows', 'column'], default='rows', help='æ‹†åˆ†æ–¹å¼')
    split_parser.add_argument('--size', type=int, default=1000, help='å—å¤§å°ï¼ˆè¡Œæ•°ï¼‰')
    split_parser.add_argument('--column', help='æ‹†åˆ†åˆ—åï¼ˆæŒ‰åˆ—æ‹†åˆ†æ—¶ä½¿ç”¨ï¼‰')
    
    # ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æŸ¥çœ‹æ–‡ä»¶ä¿¡æ¯')
    info_parser.add_argument('file_path', help='æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    converter = DataConverter()
    
    try:
        if args.command == 'convert':
            console.print(f"ğŸ”„ è½¬æ¢æ–‡ä»¶: {args.input_file} -> {args.output_file}")
            success = converter.convert_file(args.input_file, args.output_file)
            if success:
                console.print("âœ… è½¬æ¢å®Œæˆ")
            else:
                console.print("âŒ è½¬æ¢å¤±è´¥")
        
        elif args.command == 'batch':
            console.print(f"ğŸ”„ æ‰¹é‡è½¬æ¢: {args.input_dir} -> {args.output_dir}")
            result = converter.batch_convert(
                args.input_dir, args.output_dir,
                args.input_format, args.output_format
            )
            console.print(f"âœ… è½¬æ¢å®Œæˆ: {result['converted']}/{result['total_files']} ä¸ªæ–‡ä»¶")
        
        elif args.command == 'merge':
            console.print(f"ğŸ”— åˆå¹¶æ–‡ä»¶: {len(args.input_files)} ä¸ªæ–‡ä»¶ -> {args.output_file}")
            success = converter.merge_files(args.input_files, args.output_file)
            if success:
                console.print("âœ… åˆå¹¶å®Œæˆ")
            else:
                console.print("âŒ åˆå¹¶å¤±è´¥")
        
        elif args.command == 'split':
            console.print(f"âœ‚ï¸ æ‹†åˆ†æ–‡ä»¶: {args.input_file}")
            kwargs = {}
            if args.by == 'column':
                kwargs['column_name'] = args.column
            
            result = converter.split_file(
                args.input_file, args.output_dir,
                split_by=args.by, chunk_size=args.size, **kwargs
            )
            
            if result['success']:
                console.print(f"âœ… æ‹†åˆ†å®Œæˆ: {result['total_files']} ä¸ªæ–‡ä»¶")
            else:
                console.print(f"âŒ æ‹†åˆ†å¤±è´¥: {result['error']}")
        
        elif args.command == 'info':
            console.print(f"ğŸ“‹ æ–‡ä»¶ä¿¡æ¯: {args.file_path}")
            info = converter.get_file_info(args.file_path)
            
            if 'error' in info:
                console.print(f"âŒ {info['error']}")
            else:
                # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
                table = Table(title="æ–‡ä»¶ä¿¡æ¯")
                table.add_column("å±æ€§", style="cyan")
                table.add_column("å€¼", style="magenta")
                
                for key, value in info.items():
                    if key not in ['column_names', 'data_types']:
                        table.add_row(key, str(value))
                
                console.print(table)
                
                # æ˜¾ç¤ºåˆ—ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'column_names' in info:
                    console.print(f"\nğŸ“Š åˆ—ä¿¡æ¯ ({len(info['column_names'])} åˆ—):")
                    for col in info['column_names'][:10]:  # æ˜¾ç¤ºå‰10åˆ—
                        dtype = info.get('data_types', {}).get(col, 'unknown')
                        console.print(f"  â€¢ {col}: {dtype}")
                    
                    if len(info['column_names']) > 10:
                        console.print(f"  ... è¿˜æœ‰ {len(info['column_names']) - 10} åˆ—")
        
        # æ˜¾ç¤ºæŠ¥å‘Š
        converter.display_report()
        
    except KeyboardInterrupt:
        console.print("\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        console.print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()