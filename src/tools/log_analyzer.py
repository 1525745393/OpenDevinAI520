#!/usr/bin/env python3
"""
æ—¥å¿—åˆ†æå·¥å…·
æ™ºèƒ½åˆ†æåº”ç”¨æ—¥å¿—ï¼Œæå–å…³é”®ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®
"""

import os
import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import gzip
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel
from rich.tree import Tree

console = Console()


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå·¥å…·"""
    
    # å¸¸è§æ—¥å¿—çº§åˆ«
    LOG_LEVELS = ['DEBUG', 'INFO', 'WARN', 'WARNING', 'ERROR', 'FATAL', 'CRITICAL']
    
    # å¸¸è§æ—¥å¿—æ ¼å¼æ¨¡å¼
    LOG_PATTERNS = {
        'apache_common': r'(\S+) \S+ \S+ \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+|-)',
        'apache_combined': r'(\S+) \S+ \S+ \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+|-) "([^"]*)" "([^"]*)"',
        'nginx': r'(\S+) - - \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+) "([^"]*)" "([^"]*)"',
        'python_logging': r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - (\w+) - (\w+) - (.+)',
        'syslog': r'(\w{3} \d{1,2} \d{2}:\d{2}:\d{2}) (\S+) (\S+): (.+)',
        'json': r'^\{.*\}$',
        'custom_timestamp': r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d{3})?(?:Z|[+-]\d{2}:\d{2})?)',
    }
    
    # HTTPçŠ¶æ€ç åˆ†ç±»
    HTTP_STATUS_CATEGORIES = {
        '2xx': 'Success',
        '3xx': 'Redirection', 
        '4xx': 'Client Error',
        '5xx': 'Server Error'
    }
    
    def __init__(self):
        self.analyzed_files = []
        self.errors = []
        self.operations_log = []
        self.statistics = defaultdict(int)
        self.log_entries = []
        self.patterns_found = {}
    
    def detect_log_format(self, sample_lines: List[str]) -> str:
        """æ£€æµ‹æ—¥å¿—æ ¼å¼"""
        format_scores = defaultdict(int)
        
        for line in sample_lines[:100]:  # æ£€æŸ¥å‰100è¡Œ
            line = line.strip()
            if not line:
                continue
                
            for format_name, pattern in self.LOG_PATTERNS.items():
                try:
                    if re.search(pattern, line):
                        format_scores[format_name] += 1
                except re.error:
                    continue
        
        if format_scores:
            best_format = max(format_scores.items(), key=lambda x: x[1])
            return best_format[0]
        
        return 'unknown'
    
    def parse_log_line(self, line: str, log_format: str) -> Optional[Dict]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line:
            return None
        
        try:
            if log_format == 'json':
                return json.loads(line)
            
            pattern = self.LOG_PATTERNS.get(log_format)
            if not pattern:
                return {'raw': line, 'format': 'unknown'}
            
            match = re.search(pattern, line)
            if not match:
                return {'raw': line, 'format': log_format, 'parsed': False}
            
            groups = match.groups()
            
            if log_format in ['apache_common', 'apache_combined', 'nginx']:
                result = {
                    'ip': groups[0],
                    'timestamp': groups[1],
                    'method': groups[2],
                    'url': groups[3],
                    'protocol': groups[4],
                    'status': int(groups[5]),
                    'size': groups[6] if groups[6] != '-' else 0,
                    'format': log_format,
                    'parsed': True
                }
                
                if log_format in ['apache_combined', 'nginx'] and len(groups) >= 9:
                    result['referer'] = groups[7]
                    result['user_agent'] = groups[8]
                
                return result
            
            elif log_format == 'python_logging':
                return {
                    'timestamp': groups[0],
                    'level': groups[1],
                    'logger': groups[2],
                    'message': groups[3],
                    'format': log_format,
                    'parsed': True
                }
            
            elif log_format == 'syslog':
                return {
                    'timestamp': groups[0],
                    'host': groups[1],
                    'process': groups[2],
                    'message': groups[3],
                    'format': log_format,
                    'parsed': True
                }
            
            else:
                return {'raw': line, 'format': log_format, 'parsed': False}
                
        except Exception as e:
            return {'raw': line, 'error': str(e), 'parsed': False}
    
    def extract_timestamps(self, log_entry: Dict) -> Optional[datetime]:
        """æå–æ—¶é—´æˆ³"""
        if 'timestamp' not in log_entry:
            return None
        
        timestamp_str = log_entry['timestamp']
        
        # å¸¸è§æ—¶é—´æ ¼å¼
        time_formats = [
            '%Y-%m-%d %H:%M:%S,%f',  # Python logging
            '%Y-%m-%d %H:%M:%S.%f',  # ISO with microseconds
            '%Y-%m-%d %H:%M:%S',     # ISO basic
            '%Y-%m-%dT%H:%M:%S.%fZ', # ISO with Z
            '%Y-%m-%dT%H:%M:%SZ',    # ISO with Z no microseconds
            '%d/%b/%Y:%H:%M:%S %z',  # Apache format
            '%b %d %H:%M:%S',        # Syslog format
        ]
        
        for fmt in time_formats:
            try:
                return datetime.strptime(timestamp_str, fmt)
            except ValueError:
                continue
        
        # å°è¯•è‡ªåŠ¨è§£æ
        try:
            from dateutil import parser
            dt = parser.parse(timestamp_str)
            # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºæœ¬åœ°æ—¶åŒº
            if dt.tzinfo is None:
                import pytz
                dt = dt.replace(tzinfo=pytz.UTC)
            return dt
        except:
            return None
    
    def analyze_log_file(self, file_path: str, max_lines: int = None) -> Dict:
        """åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‹ç¼©æ–‡ä»¶
            if file_path.suffix == '.gz':
                open_func = gzip.open
                mode = 'rt'
            else:
                open_func = open
                mode = 'r'
            
            console.print(f"ğŸ“Š åˆ†ææ—¥å¿—æ–‡ä»¶: {file_path.name}")
            
            lines = []
            line_count = 0
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open_func(file_path, mode, encoding='utf-8', errors='ignore') as f:
                for line in f:
                    lines.append(line)
                    line_count += 1
                    if max_lines and line_count >= max_lines:
                        break
            
            if not lines:
                return {'error': 'æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–'}
            
            # æ£€æµ‹æ—¥å¿—æ ¼å¼
            log_format = self.detect_log_format(lines)
            console.print(f"ğŸ” æ£€æµ‹åˆ°æ—¥å¿—æ ¼å¼: {log_format}")
            
            # è§£ææ—¥å¿—æ¡ç›®
            parsed_entries = []
            error_count = 0
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console
            ) as progress:
                task = progress.add_task("è§£ææ—¥å¿—æ¡ç›®", total=len(lines))
                
                for line in lines:
                    progress.update(task, description=f"å·²å¤„ç†: {len(parsed_entries)} æ¡")
                    
                    entry = self.parse_log_line(line, log_format)
                    if entry:
                        if entry.get('parsed', False):
                            parsed_entries.append(entry)
                        else:
                            error_count += 1
                    
                    progress.advance(task)
            
            # ç»Ÿè®¡åˆ†æ
            analysis_result = self._analyze_entries(parsed_entries, log_format)
            analysis_result.update({
                'file_path': str(file_path),
                'file_size': file_path.stat().st_size,
                'total_lines': line_count,
                'parsed_lines': len(parsed_entries),
                'error_lines': error_count,
                'log_format': log_format,
                'parse_success_rate': f"{(len(parsed_entries) / line_count * 100):.1f}%" if line_count > 0 else "0%"
            })
            
            self.analyzed_files.append(analysis_result)
            self.operations_log.append(f"åˆ†æå®Œæˆ: {file_path.name} ({len(parsed_entries)} æ¡è®°å½•)")
            
            return analysis_result
            
        except Exception as e:
            error_msg = f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}"
            self.errors.append(error_msg)
            return {'error': error_msg}
    
    def _analyze_entries(self, entries: List[Dict], log_format: str) -> Dict:
        """åˆ†ææ—¥å¿—æ¡ç›®"""
        if not entries:
            return {'entries': []}
        
        analysis = {
            'entries': entries,
            'total_entries': len(entries),
            'time_range': {},
            'statistics': {},
            'patterns': {},
            'anomalies': []
        }
        
        # æ—¶é—´åˆ†æ
        timestamps = []
        for entry in entries:
            ts = self.extract_timestamps(entry)
            if ts:
                timestamps.append(ts)
        
        if timestamps:
            timestamps.sort()
            analysis['time_range'] = {
                'start': timestamps[0].isoformat(),
                'end': timestamps[-1].isoformat(),
                'duration': str(timestamps[-1] - timestamps[0]),
                'total_timestamps': len(timestamps)
            }
        
        # æ ¹æ®æ—¥å¿—æ ¼å¼è¿›è¡Œç‰¹å®šåˆ†æ
        if log_format in ['apache_common', 'apache_combined', 'nginx']:
            analysis['statistics'].update(self._analyze_web_logs(entries))
        elif log_format == 'python_logging':
            analysis['statistics'].update(self._analyze_python_logs(entries))
        elif log_format == 'syslog':
            analysis['statistics'].update(self._analyze_syslog(entries))
        
        # é€šç”¨æ¨¡å¼åˆ†æ
        analysis['patterns'] = self._find_patterns(entries)
        
        # å¼‚å¸¸æ£€æµ‹
        analysis['anomalies'] = self._detect_anomalies(entries, timestamps)
        
        return analysis
    
    def _analyze_web_logs(self, entries: List[Dict]) -> Dict:
        """åˆ†æWebæœåŠ¡å™¨æ—¥å¿—"""
        stats = {
            'status_codes': Counter(),
            'methods': Counter(),
            'ips': Counter(),
            'urls': Counter(),
            'user_agents': Counter(),
            'total_bytes': 0,
            'unique_ips': set(),
            'error_rate': 0
        }
        
        for entry in entries:
            if 'status' in entry:
                stats['status_codes'][entry['status']] += 1
            if 'method' in entry:
                stats['methods'][entry['method']] += 1
            if 'ip' in entry:
                stats['ips'][entry['ip']] += 1
                stats['unique_ips'].add(entry['ip'])
            if 'url' in entry:
                stats['urls'][entry['url']] += 1
            if 'user_agent' in entry:
                stats['user_agents'][entry['user_agent']] += 1
            if 'size' in entry and isinstance(entry['size'], int):
                stats['total_bytes'] += entry['size']
        
        # è®¡ç®—é”™è¯¯ç‡
        total_requests = len(entries)
        error_requests = sum(count for status, count in stats['status_codes'].items() 
                           if status >= 400)
        stats['error_rate'] = f"{(error_requests / total_requests * 100):.2f}%" if total_requests > 0 else "0%"
        
        # è½¬æ¢é›†åˆä¸ºæ•°é‡
        stats['unique_ips'] = len(stats['unique_ips'])
        
        return stats
    
    def _analyze_python_logs(self, entries: List[Dict]) -> Dict:
        """åˆ†æPythonåº”ç”¨æ—¥å¿—"""
        stats = {
            'log_levels': Counter(),
            'loggers': Counter(),
            'messages': Counter(),
            'error_messages': [],
            'warning_messages': []
        }
        
        for entry in entries:
            if 'level' in entry:
                stats['log_levels'][entry['level']] += 1
            if 'logger' in entry:
                stats['loggers'][entry['logger']] += 1
            if 'message' in entry:
                message = entry['message']
                stats['messages'][message] += 1
                
                # æ”¶é›†é”™è¯¯å’Œè­¦å‘Šæ¶ˆæ¯
                level = entry.get('level', '').upper()
                if level in ['ERROR', 'CRITICAL', 'FATAL']:
                    stats['error_messages'].append(message)
                elif level in ['WARNING', 'WARN']:
                    stats['warning_messages'].append(message)
        
        return stats
    
    def _analyze_syslog(self, entries: List[Dict]) -> Dict:
        """åˆ†æç³»ç»Ÿæ—¥å¿—"""
        stats = {
            'hosts': Counter(),
            'processes': Counter(),
            'messages': Counter(),
            'message_patterns': Counter()
        }
        
        for entry in entries:
            if 'host' in entry:
                stats['hosts'][entry['host']] += 1
            if 'process' in entry:
                stats['processes'][entry['process']] += 1
            if 'message' in entry:
                message = entry['message']
                stats['messages'][message] += 1
                
                # æå–æ¶ˆæ¯æ¨¡å¼
                pattern = re.sub(r'\d+', 'N', message)  # æ›¿æ¢æ•°å­—
                pattern = re.sub(r'\b\w{32,}\b', 'HASH', pattern)  # æ›¿æ¢é•¿å­—ç¬¦ä¸²
                stats['message_patterns'][pattern] += 1
        
        return stats
    
    def _find_patterns(self, entries: List[Dict]) -> Dict:
        """æŸ¥æ‰¾æ—¥å¿—æ¨¡å¼"""
        patterns = {
            'repeated_messages': Counter(),
            'ip_patterns': Counter(),
            'time_patterns': Counter()
        }
        
        # æŸ¥æ‰¾é‡å¤æ¶ˆæ¯
        for entry in entries:
            if 'message' in entry:
                patterns['repeated_messages'][entry['message']] += 1
            elif 'url' in entry:
                patterns['repeated_messages'][entry['url']] += 1
        
        # åªä¿ç•™å‡ºç°å¤šæ¬¡çš„æ¨¡å¼
        patterns['repeated_messages'] = {
            msg: count for msg, count in patterns['repeated_messages'].items() 
            if count > 1
        }
        
        return patterns
    
    def _detect_anomalies(self, entries: List[Dict], timestamps: List[datetime]) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        if not timestamps:
            return anomalies
        
        # æ£€æµ‹æ—¶é—´é—´éš”å¼‚å¸¸
        if len(timestamps) > 1:
            intervals = []
            for i in range(1, len(timestamps)):
                interval = (timestamps[i] - timestamps[i-1]).total_seconds()
                intervals.append(interval)
            
            if intervals:
                avg_interval = sum(intervals) / len(intervals)
                for i, interval in enumerate(intervals):
                    if interval > avg_interval * 10:  # é—´éš”è¶…è¿‡å¹³å‡å€¼10å€
                        anomalies.append({
                            'type': 'time_gap',
                            'description': f'æ—¶é—´é—´éš”å¼‚å¸¸: {interval:.2f}ç§’',
                            'timestamp': timestamps[i+1].isoformat(),
                            'severity': 'medium'
                        })
        
        # æ£€æµ‹çŠ¶æ€ç å¼‚å¸¸ï¼ˆé’ˆå¯¹Webæ—¥å¿—ï¼‰
        status_codes = [entry.get('status') for entry in entries if 'status' in entry]
        if status_codes:
            error_codes = [code for code in status_codes if code >= 500]
            if len(error_codes) > len(status_codes) * 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                anomalies.append({
                    'type': 'high_error_rate',
                    'description': f'é«˜é”™è¯¯ç‡: {len(error_codes)}/{len(status_codes)} ({len(error_codes)/len(status_codes)*100:.1f}%)',
                    'severity': 'high'
                })
        
        return anomalies
    
    def batch_analyze(self, directory: str, pattern: str = "*.log", 
                     max_files: int = None, max_lines_per_file: int = None) -> Dict:
        """æ‰¹é‡åˆ†ææ—¥å¿—æ–‡ä»¶"""
        directory_path = Path(directory)
        if not directory_path.exists():
            error_msg = f"ç›®å½•ä¸å­˜åœ¨: {directory}"
            self.errors.append(error_msg)
            return {'analyzed': [], 'errors': [error_msg]}
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_files = list(directory_path.glob(pattern))
        if pattern != "*.log":
            # ä¹ŸæŸ¥æ‰¾å‹ç¼©æ–‡ä»¶
            log_files.extend(directory_path.glob(pattern + ".gz"))
        
        if not log_files:
            error_msg = f"åœ¨ç›®å½• {directory} ä¸­æœªæ‰¾åˆ°åŒ¹é… {pattern} çš„æ—¥å¿—æ–‡ä»¶"
            self.errors.append(error_msg)
            return {'analyzed': [], 'errors': [error_msg]}
        
        if max_files:
            log_files = log_files[:max_files]
        
        console.print(f"ğŸ“ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        analyzed_results = []
        errors = []
        
        for file_path in log_files:
            try:
                result = self.analyze_log_file(str(file_path), max_lines_per_file)
                if 'error' in result:
                    errors.append(result['error'])
                else:
                    analyzed_results.append(result)
            except Exception as e:
                error_msg = f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {str(e)}"
                errors.append(error_msg)
                self.errors.append(error_msg)
        
        return {
            'analyzed': analyzed_results,
            'errors': errors,
            'total_files': len(log_files),
            'success_count': len(analyzed_results),
            'error_count': len(errors)
        }
    
    def generate_report(self, output_file: str = None) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.analyzed_files:
            return {'error': 'æ²¡æœ‰åˆ†æç»“æœå¯ç”ŸæˆæŠ¥å‘Š'}
        
        report = {
            'summary': {
                'total_files': len(self.analyzed_files),
                'total_entries': sum(f.get('total_entries', 0) for f in self.analyzed_files),
                'total_errors': len(self.errors),
                'analysis_time': datetime.now().isoformat()
            },
            'files': self.analyzed_files,
            'global_statistics': self._generate_global_stats(),
            'recommendations': self._generate_recommendations()
        }
        
        if output_file:
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                console.print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
            except Exception as e:
                self.errors.append(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        
        return report
    
    def _generate_global_stats(self) -> Dict:
        """ç”Ÿæˆå…¨å±€ç»Ÿè®¡"""
        global_stats = {
            'total_log_entries': 0,
            'log_formats': Counter(),
            'time_range': {},
            'common_patterns': Counter()
        }
        
        all_timestamps = []
        
        for file_result in self.analyzed_files:
            global_stats['total_log_entries'] += file_result.get('total_entries', 0)
            global_stats['log_formats'][file_result.get('log_format', 'unknown')] += 1
            
            # æ”¶é›†æ—¶é—´èŒƒå›´
            time_range = file_result.get('time_range', {})
            if time_range.get('start'):
                try:
                    start_time = datetime.fromisoformat(time_range['start'])
                    end_time = datetime.fromisoformat(time_range['end'])
                    all_timestamps.extend([start_time, end_time])
                except:
                    pass
        
        if all_timestamps:
            # ç¡®ä¿æ‰€æœ‰æ—¶é—´æˆ³éƒ½æœ‰æ—¶åŒºä¿¡æ¯
            normalized_timestamps = []
            for ts in all_timestamps:
                if ts.tzinfo is None:
                    import pytz
                    ts = ts.replace(tzinfo=pytz.UTC)
                normalized_timestamps.append(ts)
            
            normalized_timestamps.sort()
            global_stats['time_range'] = {
                'start': normalized_timestamps[0].isoformat(),
                'end': normalized_timestamps[-1].isoformat(),
                'total_duration': str(normalized_timestamps[-1] - normalized_timestamps[0])
            }
        
        return global_stats
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        for file_result in self.analyzed_files:
            stats = file_result.get('statistics', {})
            
            # Webæ—¥å¿—å»ºè®®
            if 'error_rate' in stats:
                error_rate = float(stats['error_rate'].rstrip('%'))
                if error_rate > 5:
                    recommendations.append(f"é«˜é”™è¯¯ç‡è­¦å‘Š: {stats['error_rate']} - å»ºè®®æ£€æŸ¥åº”ç”¨ç¨‹åºé”™è¯¯")
            
            # å¼‚å¸¸æ£€æµ‹å»ºè®®
            anomalies = file_result.get('anomalies', [])
            if anomalies:
                high_severity = [a for a in anomalies if a.get('severity') == 'high']
                if high_severity:
                    recommendations.append("æ£€æµ‹åˆ°é«˜ä¸¥é‡æ€§å¼‚å¸¸ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥")
        
        if not recommendations:
            recommendations.append("æ—¥å¿—åˆ†ææ­£å¸¸ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        
        return recommendations
    
    def display_summary(self):
        """æ˜¾ç¤ºåˆ†ææ‘˜è¦"""
        if not self.analyzed_files:
            console.print("âŒ æ²¡æœ‰åˆ†æç»“æœ")
            return
        
        # åˆ›å»ºæ‘˜è¦è¡¨æ ¼
        table = Table(title="ğŸ“Š æ—¥å¿—åˆ†ææ‘˜è¦")
        table.add_column("æ–‡ä»¶", style="cyan")
        table.add_column("æ ¼å¼", style="magenta")
        table.add_column("æ¡ç›®æ•°", style="green")
        table.add_column("è§£æç‡", style="yellow")
        table.add_column("çŠ¶æ€", style="blue")
        
        for result in self.analyzed_files:
            status = "âœ… æˆåŠŸ" if 'error' not in result else "âŒ å¤±è´¥"
            table.add_row(
                Path(result.get('file_path', '')).name,
                result.get('log_format', 'unknown'),
                str(result.get('total_entries', 0)),
                result.get('parse_success_rate', '0%'),
                status
            )
        
        console.print(table)
        
        # æ˜¾ç¤ºå…¨å±€ç»Ÿè®¡
        global_stats = self._generate_global_stats()
        console.print(f"\nğŸ“ˆ æ€»è®¡: {global_stats['total_log_entries']} æ¡æ—¥å¿—è®°å½•")
        console.print(f"ğŸ•’ æ—¶é—´èŒƒå›´: {global_stats.get('time_range', {}).get('total_duration', 'N/A')}")
        
        # æ˜¾ç¤ºå»ºè®®
        recommendations = self._generate_recommendations()
        if recommendations:
            console.print("\nğŸ’¡ å»ºè®®:")
            for rec in recommendations:
                console.print(f"  â€¢ {rec}")
    
    def get_report(self) -> Dict:
        """è·å–åˆ†ææŠ¥å‘Š"""
        return {
            'analyzed_files': self.analyzed_files,
            'errors': self.errors,
            'operations_log': self.operations_log,
            'total_analyzed': len(self.analyzed_files),
            'total_errors': len(self.errors)
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥å¿—åˆ†æå·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ†æå•ä¸ªæ–‡ä»¶
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶')
    analyze_parser.add_argument('file_path', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--max-lines', type=int, help='æœ€å¤§åˆ†æè¡Œæ•°')
    analyze_parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    # æ‰¹é‡åˆ†æ
    batch_parser = subparsers.add_parser('batch', help='æ‰¹é‡åˆ†ææ—¥å¿—æ–‡ä»¶')
    batch_parser.add_argument('directory', help='æ—¥å¿—æ–‡ä»¶ç›®å½•')
    batch_parser.add_argument('--pattern', default='*.log', help='æ–‡ä»¶åŒ¹é…æ¨¡å¼')
    batch_parser.add_argument('--max-files', type=int, help='æœ€å¤§æ–‡ä»¶æ•°')
    batch_parser.add_argument('--max-lines', type=int, help='æ¯ä¸ªæ–‡ä»¶æœ€å¤§åˆ†æè¡Œæ•°')
    batch_parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    # å®æ—¶ç›‘æ§
    monitor_parser = subparsers.add_parser('monitor', help='å®æ—¶ç›‘æ§æ—¥å¿—æ–‡ä»¶')
    monitor_parser.add_argument('file_path', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    monitor_parser.add_argument('--tail', type=int, default=10, help='æ˜¾ç¤ºæœ€åNè¡Œ')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    analyzer = LogAnalyzer()
    
    try:
        if args.command == 'analyze':
            console.print(f"ğŸ“Š åˆ†ææ—¥å¿—æ–‡ä»¶: {args.file_path}")
            result = analyzer.analyze_log_file(args.file_path, args.max_lines)
            
            if 'error' in result:
                console.print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
            else:
                analyzer.display_summary()
                
                if args.output:
                    analyzer.generate_report(args.output)
        
        elif args.command == 'batch':
            console.print(f"ğŸ“ æ‰¹é‡åˆ†æ: {args.directory}")
            result = analyzer.batch_analyze(
                args.directory, 
                args.pattern, 
                args.max_files, 
                args.max_lines
            )
            
            console.print(f"âœ… åˆ†æå®Œæˆ: {result['success_count']}/{result['total_files']} ä¸ªæ–‡ä»¶")
            
            if result['errors']:
                console.print(f"âŒ é”™è¯¯: {len(result['errors'])} ä¸ª")
                for error in result['errors'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    console.print(f"  â€¢ {error}")
            
            analyzer.display_summary()
            
            if args.output:
                analyzer.generate_report(args.output)
        
        elif args.command == 'monitor':
            console.print(f"ğŸ‘ï¸ ç›‘æ§æ—¥å¿—æ–‡ä»¶: {args.file_path}")
            console.print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
            
            # ç®€å•çš„tailå®ç°
            file_path = Path(args.file_path)
            if not file_path.exists():
                console.print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file_path}")
                return
            
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    # è·³åˆ°æ–‡ä»¶æœ«å°¾
                    f.seek(0, 2)
                    
                    while True:
                        line = f.readline()
                        if line:
                            # ç®€å•è§£æå¹¶æ˜¾ç¤º
                            entry = analyzer.parse_log_line(line, 'unknown')
                            if entry and entry.get('parsed'):
                                console.print(f"[{datetime.now().strftime('%H:%M:%S')}] {line.strip()}")
                            else:
                                console.print(f"[{datetime.now().strftime('%H:%M:%S')}] {line.strip()}")
                        else:
                            import time
                            time.sleep(0.1)
            except KeyboardInterrupt:
                console.print("\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
        
    except KeyboardInterrupt:
        console.print("\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
    except Exception as e:
        console.print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()